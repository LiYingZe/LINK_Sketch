# ğŸš€ **The Zero-Shot Semantic Sketch**

Approximate frequency counting plays a vital role in analyzing large-scale corpora ğŸ“š, especially in the context of training and curating datasets for Large Language Models (LLMs) ğŸ¤–. Traditional approachesâ€”such as sketch-based and sampling-based methodsâ€”often struggle in enterprise-scale settings due to high computational overhead ğŸ’» or poor accuracy when data access is limited ğŸ”’.

Recent advances in LLMs have opened up new possibilities for frequency estimation using only minimal data access and natural language dataset descriptions ğŸ’¡. However, LLMs are prone to hallucinations ğŸŒ€ and inherent biases ğŸ­, which can significantly undermine the reliability of their estimatesâ€”particularly when applied to domain-specific or niche corpora ğŸ¯.

------

### ğŸ”— Meet **LINK**: *Language-Informed Neural Knowledge* ğŸ§ âœ¨

![LINK](./LINK.png)

LINK is a novel estimator that combines **sparse data-level statistics** ğŸ“Š with **semantic-level dataset descriptions** ğŸ“ for comprehensive frequency estimation. Data-level augmented signals serve as unbiased anchors âš“ for both enhancement signals and hallucinations, effectively reducing LLM hallucinations and biases ğŸ›¡ï¸. Semantic-level dataset summarization provides condensed features beyond sparse data ğŸ—œï¸, offering additional global frequency information that enables more accurate frequency estimation ğŸ¯.

### ğŸ† Why LINK?

- ğŸ¯ **Unbiased Anchoring**: Sparse data-level statistics eliminate LLM hallucinations
- ğŸ§  **Semantic Intelligence**: Natural language descriptions provide global context
- ğŸ“Š **Enterprise-Scale**: Handles massive corpora with minimal computational overhead
- ğŸ”’ **Limited Access Friendly**: Works efficiently with restricted data access
- ğŸ­ **Bias Reduction**: Effectively mitigates inherent LLM biases
- âš¡ **Zero-Shot Capability**: No training required on target datasets

------

## ğŸ—‚ï¸ Code Structure

Our project is organized as follows:

```
.
â”œâ”€â”€ data                           ğŸ“ Dataset collection
â”‚   â”œâ”€â”€ Amazon                     ğŸ›’ E-commerce dataset
â”‚   â”‚   â”œâ”€â”€ download.py            ğŸ“¥ Download script
â”‚   â”‚   â””â”€â”€ parse.py               ğŸ” Parser utility
â”‚   â”œâ”€â”€ Children                   ğŸ‘¶ Children's literature dataset
â”‚   â”‚   â”œâ”€â”€ download.py            ğŸ“¥ Download script
â”‚   â”‚   â””â”€â”€ parse.py               ğŸ” Parser utility
â”‚   â”œâ”€â”€ Finance                    ğŸ’° Financial corpus dataset
â”‚   â”‚   â”œâ”€â”€ download.py            ğŸ“¥ Download script
â”‚   â”‚   â””â”€â”€ parse.py               ğŸ” Parser utility
â”‚   â””â”€â”€ Math                       ğŸ”¢ Mathematical text dataset
â”‚       â”œâ”€â”€ download.py            ğŸ“¥ Download script
â”‚       â””â”€â”€ parse.py               ğŸ” Parser utility
â”œâ”€â”€ cache                          ğŸ’¾ Processed frequency dictionaries
â”œâ”€â”€ main.py                        ğŸš€ Main evaluation program
â”œâ”€â”€ interactive_chat.py            ğŸ’¬ Interactive query interface
â”œâ”€â”€ generate_cache.py              ğŸ› ï¸ Cache generation utility
â”œâ”€â”€ dataset_config.txt             ğŸ“‹ Dataset configuration and statistics
â””â”€â”€ requirements.txt               ğŸ“‹ Dependencies list
```

------

## ğŸ“¦ Requirements

- ğŸ”„ python==3.11.0
- ğŸ“š datasets==2.19.0
- ğŸ¤— huggingface-hub==0.27.1
- ğŸ”¢ numpy==1.26.4
- ğŸ¤– openai==1.74.0

------

## ğŸ“¥ Setup & Dataset

We provide **4 HuggingFace corpus datasets** (Amazon, Children, Finance, Math) ğŸ¯. To use them, you first need to download the datasets from HuggingFace ğŸ¤—.

### ğŸ“š Dataset Download

Taking the **Children** dataset as an example, use the following commands:

```bash
huggingface-cli login    # ğŸ” Login to your HuggingFace account for access
python ./data/Children/download.py    # ğŸ“¥ Download dataset from HuggingFace
```

âœ… **Repeat this process** for other datasets (Amazon, Finance, Math) by navigating to their respective directories! ğŸ”„

------

## ğŸ’¾ Generate Dataset Cache

Parse the downloaded datasets and convert files into frequency dictionaries ğŸ“Š for convenient subsequent operations. Taking the **Children** dataset as an example, use the following commands:

```bash
# ğŸ“ˆ Get complete frequency information
python ./generate_cache.py --datasetName "Children" --Num_In 800000 --SampleRate 1 --SaveName "Children_Full"

# ğŸ² Get sampled frequency information  
python ./generate_cache.py --datasetName "Children" --Num_In 800000 --SampleRate 1e-5 --SaveName "Children_Sample1e-5"
```

ğŸ‰ **Now your dataset is loaded and parsed!** Ready for the next steps! ğŸš€

------

## ğŸ§ª Test LINK

Our main program can be used to test LINK's estimation performance on different datasets ğŸ“Š, using **Q-error** as the evaluation metric ğŸ“. You need to specify dataset parameters (`--datasetName`, `--Num_In`, `--datasetDescription`) in the arguments or use default parameters ğŸ› ï¸. All dataset statistical information is available in the `dataset_config.txt` file ğŸ“‹.

Run the following command with default parameters:

```bash
python ./main.py
```

Or customize with specific dataset parameters:

```bash
python ./main.py --datasetName "Children" --Num_In 800000 --datasetDescription "..."
```

ğŸ” This evaluates LINK's frequency estimation accuracy across configured datasets and provides comprehensive performance metrics! All dataset configurations and statistical information can be found in `dataset_config.txt` ğŸ“ˆâœ¨

------

## ğŸ’¬ Interactive Word Frequency Prediction ğŸ¤–ğŸ”

We provide an **interactive program** ğŸ•¹ï¸ that supports specific word frequency queries ğŸ”. Simply add the `--word` parameter in the command line to predict word frequency through LINK ğŸ¯. You also need to specify which dataset to use in the parameters ğŸ“‹.

### ğŸ§™ Try it Yourself!

Taking the most common word **"the"** as an example with the Children dataset:

```bash
python ./interactive_chat.py --word "the" --datasetName "Children"
```

Or test with different datasets:

```bash
python ./interactive_chat.py --word "investment" --datasetName "Finance" --Num_In 518815 --datasetDescription "..."
python ./interactive_chat.py --word "algorithm" --datasetName "Math" --Num_In 6497564 --datasetDescription "..."
```

------

### ğŸ® What it does:

- ğŸ—£ï¸ **Converse with LINK**: Effortlessly query word frequencies in natural language
- âš¡ **Instantaneous Results**: Get frequency predictions in real-time
- ğŸ¯ **Semantic Understanding**: Leverages dataset descriptions for accurate estimation
- ğŸ’¡ **Zero-Shot Magic**: No training required on target datasets
- ğŸ“Š **Enterprise-Scale**: Handles massive corpora efficiently
- ğŸ›¡ï¸ **Bias-Free**: Reduces LLM hallucinations with data-level anchoring

------

### ğŸ¯ Sample Queries to Try

Here are some interesting words you can test:

- `"artificial"` - Technology domain ğŸ¤–
- `"investment"` - Finance domain ğŸ’°  
- `"learning"` - Education domain ğŸ“š
- `"algorithm"` - Computer science domain ğŸ’»
- `"children"` - Literature domain ğŸ‘¶

> ğŸ§  *Behind the scenes*: LINK combines sparse data statistics with semantic dataset understanding to provide accurate frequency estimatesâ€”**way smarter** than traditional sketch-based methods! ğŸ”¥

------

Whether you're analyzing corpus statistics, curating LLM datasets, or exploring word distributionsâ€”LINK makes frequency estimation **intelligent and reliable**! ğŸ¤“ğŸ‰

Ready to **link** your data with semantic intelligence? ğŸ”—âœ¨ Let the zero-shot frequency magic begin! âš¡ğŸ§ ğŸ“Š

------

**LINK** revolutionizes frequency estimation by bridging the gap between sparse data access and semantic understanding ğŸŒ‰. Say goodbye to hallucinations and hello to **reliable, enterprise-scale frequency counting**! ğŸ¯ğŸš€

Ready to make your frequency estimates **smarter**? ğŸ˜„ Let neural semantic sketching begin! âš¡ğŸ¤–ğŸ“ˆ